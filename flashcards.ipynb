{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Fg61OHJEE3M",
        "outputId": "601d7620-15a3-4ac1-e3ee-ac2e4bb079b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "n8q_zkXpEa98"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qbwO_0B1FSCt",
        "outputId": "94b4eb2b-1e48-4e2e-a7a3-14e23130798c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token has not been saved to git credential helper.\n",
            "WARNING:huggingface_hub._login:Token has not been saved to git credential helper.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "  You are an specialized tutor in creating flashcards about whatever topic the user decides to research.\n",
        "  They need to be brief, with a short question and a short answer in the following markdown format example\n",
        "  ###TEMPLATE###\n",
        "  # Flashcard 1\n",
        "  <details>\n",
        "  <summary>What is the capital of France?</summary>\n",
        "  Paris\n",
        "  </details>\n",
        "\n",
        "  # Flashcard 2\n",
        "  <details>\n",
        "  <summary>What is the derivative of sin(x)?</summary>\n",
        "  cos(x)\n",
        "  </details>\n",
        "  ###TEMPLATE###\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "c7hIyb9LIjEr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "RSiQjN97OGqJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, messages, quant=True, stream=True, max_new_tokens=500):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  if quant:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n",
        "  else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n",
        "  if stream:\n",
        "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
        "  else:\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens)\n",
        "\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  return response"
      ],
      "metadata": {
        "id": "jZ0m7i22OnnR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "def call_generate(model_name, topic, number_flashcards):\n",
        "  if model_name == \"LLAMA\":\n",
        "    model = LLAMA\n",
        "  elif model_name == \"DEEPSEEK\":\n",
        "    model = DEEPSEEK\n",
        "  else:\n",
        "    return \"Invalid model selected.\"\n",
        "\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": system_message},\n",
        "      {\"role\": \"user\", \"content\": f\"I want to more about {topic}. Please provide {number_flashcards} flashcards.\"}\n",
        "  ]\n",
        "\n",
        "  reponse = generate(model, messages, stream=False, max_new_tokens=2000)\n",
        "  text = re.sub(r'###TEMPLATE.*?###TEMPLATE', '', reponse, flags=re.DOTALL)\n",
        "\n",
        "  result = re.search(r\"(# Flashcard 1[s\\S\\]*</details>)\", text)\n",
        "\n",
        "  if result:\n",
        "    response = result.group(1)\n",
        "  else:\n",
        "    response\n",
        "  return response\n",
        "\n",
        "with gr.Blocks as ui:\n",
        "  with gr.Row():\n",
        "    model_dropdown = gr.Dropdown(choices=[\"LLAMA\", \"DEEPSEEK\"], value=\"LLAMA\", label=\"Model\")\n",
        "  with gr.Row():\n",
        "    topic_selector = gr.Textbox(label=\"Type the topic you want flashcards:\", max_lines=1, max_length=50)\n",
        "    number_flashcards = gr.Slider (\n",
        "        minimum=1,\n",
        "        maximum=10,\n",
        "        step=1,\n",
        "        value=1,\n",
        "        label=\"number flashcards\"\n",
        "    )\n",
        "  with gr.Row():\n",
        "    generate_button = gr.Button(\"Generate flashcards\")\n",
        "  with gr.Row():\n",
        "    output = gr.Markdown()\n",
        "\n",
        "  generate_button.click(\n",
        "      call_generate,\n",
        "      inputs=[model_dropdown, topic_selector, number_flashcards],\n",
        "      outputs=output\n",
        "  )\n",
        "\n",
        "ui.launch(inbrowser=True, debug=True)"
      ],
      "metadata": {
        "id": "kSaPonRuVwEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}